{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b744f517",
   "metadata": {},
   "source": [
    "converting .txt policy documents to chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c7e3b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aimte\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def sentence_chunk_policy_docs(folder_path, chunk_size=3):\n",
    "    chunks = []\n",
    "    for file in Path(folder_path).glob(\"*.txt\"):\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        sentences = sent_tokenize(text)\n",
    "        for i in range(0, len(sentences), chunk_size):\n",
    "            chunk = \" \".join(sentences[i:i + chunk_size])\n",
    "            chunks.append({\"content\": chunk, \"source\": file.name})\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68e794f",
   "metadata": {},
   "source": [
    "embed chunks using a sentence embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c3eedfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Get embeddings for chunks\n",
    "chunks = sentence_chunk_policy_docs(\"policy_doc\")\n",
    "texts = [chunk[\"content\"] for chunk in chunks]\n",
    "embeddings = embedder.encode(texts, convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7234f654",
   "metadata": {},
   "source": [
    "saving the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f0f1013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Save chunks\n",
    "with open(\"policy_chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee6d0c8",
   "metadata": {},
   "source": [
    "store and query using faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6fbdee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Convert embeddings to numpy\n",
    "embedding_matrix = np.array([emb.numpy() for emb in embeddings])\n",
    "\n",
    "# Build FAISS index\n",
    "index = faiss.IndexFlatL2(embedding_matrix.shape[1])\n",
    "index.add(embedding_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b1508a",
   "metadata": {},
   "source": [
    "saving FAISS index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d29307d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"policy_index.faiss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fa907a",
   "metadata": {},
   "source": [
    "loading faiss index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6fb98410",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.read_index(\"policy_index.faiss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6518aca0",
   "metadata": {},
   "source": [
    "to query relevant policy chunks for a claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f33043fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(claim_text, k=3):\n",
    "    query_embedding = embedder.encode([claim_text])[0]\n",
    "    distances, indices = index.search(np.array([query_embedding]), k)\n",
    "    return [chunks[i] for i in indices[0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5426bb",
   "metadata": {},
   "source": [
    "comnine reteived context+claim->feed into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02c8114d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model and tokenizer loaded from local directory.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "df = pd.read_csv(\"claims_final.csv\", quotechar='\"')\n",
    "df[\"SeverityEncoded\"] = label_encoder.fit_transform(df[\"SeverityLabel\"])\n",
    "\n",
    "\n",
    "# Local directory where model was saved\n",
    "model_save_path = \"roberta_severity_model\"\n",
    "\n",
    "# Load tokenizer and model from local folder\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_save_path)\n",
    "classifer = RobertaForSequenceClassification.from_pretrained(model_save_path)\n",
    "classifer.eval()\n",
    "print(\"✅ Model and tokenizer loaded from local directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def predict_with_context(claim_text):\n",
    "    retrieved = retrieve_relevant_chunks(claim_text)\n",
    "    context = \" \".join([chunk[\"content\"] for chunk in retrieved])\n",
    "    \n",
    "    # Combine context and claim\n",
    "    # input_text = f\"[CLAIM]: {claim_text}\"\n",
    "\n",
    "    inputs = tokenizer(claim_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = classifer(**inputs)\n",
    "        predicted_class = torch.argmax(outputs.logits, dim=1).item()\n",
    "    predicted_label = label_encoder.inverse_transform([predicted_class])[0]\n",
    "\n",
    "    return predicted_label,retrieved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ff4549",
   "metadata": {},
   "source": [
    "Test the full RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claim: my eye got stabbed\n",
      "Predicted Severity Label: High\n",
      "\n",
      "Top Retrieved Policy Chunks:\n",
      "\n",
      "--- Chunk 1 from POL002_Laceration_Injuries.txt ---\n",
      "Policy Name: Laceration and Cut Injuries (POL002)\n",
      "\n",
      "This policy covers accidental cuts, punctures, or lacerations sustained in the workplace while using tools, equipment, or materials. Commonly covered scenarios include injuries from utility knives, glass, or sharp machine edges. To be eligible, the injury must be promptly reported (within 24 hours) and documented with a supervisor's report and a clinical evaluation.\n",
      "\n",
      "--- Chunk 2 from POL006_Repetitive_Stress.txt ---\n",
      "Policy Name: Repetitive Stress Injuries (POL006)\n",
      "\n",
      "Coverage includes injuries caused by repetitive movements such as typing, scanning, lifting, or tool use. These include carpal tunnel syndrome, tendonitis, and joint inflammation. A medical history review and work activity log must accompany the claim.\n",
      "\n",
      "--- Chunk 3 from POL004_Falling_Object_Injuries.txt ---\n",
      "Policy Name: Falling Object Injury Coverage (POL004)\n",
      "\n",
      "This policy provides protection for injuries caused by objects falling from shelves, scaffolding, or overhead fixtures. Common injuries include head trauma, contusions, or fractures. All employees must wear hard hats and follow safety protocols in designated zones.\n"
     ]
    }
   ],
   "source": [
    "claim = \"my eye got stabbed\"\n",
    "label, retrieved_chunks = predict_with_context(claim)\n",
    "print(\"claim:\", claim)\n",
    "print(\"Predicted Severity Label:\", label)\n",
    "\n",
    "print(\"\\nTop Retrieved Policy Chunks:\")\n",
    "for i, chunk in enumerate(retrieved_chunks, 1):\n",
    "    print(f\"\\n--- Chunk {i} from {chunk['source']} ---\\n{chunk['content']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
